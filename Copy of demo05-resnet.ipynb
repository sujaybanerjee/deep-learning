{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/rtealwitter/dl-demos/blob/main/demo05-resnet.ipynb","timestamp":1673550365521}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5deff87f65ce4fd28533c66e84702548":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fea8a65789de444594527251bed8d116","IPY_MODEL_ae4252c072a84be39fbf5e30402aacd5","IPY_MODEL_df25980605cd41d5aea3ddcfe8e72b07"],"layout":"IPY_MODEL_1a1e6942bce84b68b7d73e3258503b11"}},"fea8a65789de444594527251bed8d116":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7010af1383484ac8897357d6209e4a18","placeholder":"​","style":"IPY_MODEL_5f36225fbc4343dc88da438a656eb3b3","value":"100%"}},"ae4252c072a84be39fbf5e30402aacd5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89ee8c30bab34dcca776d4529cfa0c7d","max":87319819,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e95b6d411c3a47ea8c0ccad2699330a2","value":87319819}},"df25980605cd41d5aea3ddcfe8e72b07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cda7d8c7d6dd42d2a451b224d9a43976","placeholder":"​","style":"IPY_MODEL_5444e6cf6e5746a0b764a0826b8a1521","value":" 83.3M/83.3M [00:00&lt;00:00, 200MB/s]"}},"1a1e6942bce84b68b7d73e3258503b11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7010af1383484ac8897357d6209e4a18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f36225fbc4343dc88da438a656eb3b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89ee8c30bab34dcca776d4529cfa0c7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e95b6d411c3a47ea8c0ccad2699330a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cda7d8c7d6dd42d2a451b224d9a43976":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5444e6cf6e5746a0b764a0826b8a1521":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"fc9wyyGloQwL"},"source":["# Finetuning a ResNet\n","\n","Modern deep convnets tend to have tens (if not hundreds) of layers, with millions (if not tens of millions) of trainable parameters. More often than not several of these layers have skipped connections; the *ResNet* family of networks are an example. \n","\n","The flip side of having such deep network architectures is that to properly learn such networks, one requires *massive* amounts of training data. In most applications, access to such massive datasets simply isn't available; gathering and curating a dataset with a few hundred/thousand examples itself can be a challenge.\n","\n","What should one do in the \"small data\" setting? A possible solution:\n","* start with a deep network architecture initialized with *pre-trained* weights, and\n","* *fine-tune* the network weights on the (small) training dataset.\n","\n","In this demo we will see how to train a simple cat-vs-dog classifier using a very small training dataset of 60 images. The dataset is provided [here](https://github.com/chinmayhegde/dl-demos/blob/main/data.zip). You can unzip and save the dataset anywhere you like; I've saved it to my Google drive folder: `MyDrive/data`.\n"]},{"cell_type":"code","metadata":{"id":"bGT3H4DjoO9n","executionInfo":{"status":"ok","timestamp":1675110577175,"user_tz":300,"elapsed":6768,"user":{"displayName":"Sujay Banerjee","userId":"05482044087577137222"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Since I put the data in `MyDrive/data`, I'll have to give colab access to the folder so we can load it."],"metadata":{"id":"0UQfZh_XFrpV"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"pMIhxViRFP1s","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3fdc4a9-46ef-458b-8d28-7f9a453c0892","executionInfo":{"status":"ok","timestamp":1675110606005,"user_tz":300,"elapsed":28838,"user":{"displayName":"Sujay Banerjee","userId":"05482044087577137222"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"qNHnEAckpqps","executionInfo":{"status":"ok","timestamp":1675110617484,"user_tz":300,"elapsed":1446,"user":{"displayName":"Sujay Banerjee","userId":"05482044087577137222"}}},"source":["\n","# data transforms\n","dset_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])])\n","\n","\n","# Use the image folder function to create datasets\n","dsets = {x: datasets.ImageFolder(f\"/content/drive/MyDrive/Deep Learning training data/{x}\", dset_transform)\n","         for x in ['train', 'val']}\n","\n","# create data loader\n","dataloaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=16,\n","                                              shuffle=(x == \"train\"))\n","               for x in ['train', 'val']}\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","source":["dsets"],"metadata":{"id":"7gvep_F7ZXF8","executionInfo":{"status":"ok","timestamp":1675111009829,"user_tz":300,"elapsed":180,"user":{"displayName":"Sujay Banerjee","userId":"05482044087577137222"}},"outputId":"67514588-1e20-4bc8-b9b8-868de8c82833","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'train': Dataset ImageFolder\n","     Number of datapoints: 60\n","     Root location: /content/drive/MyDrive/Deep Learning training data/train\n","     StandardTransform\n"," Transform: Compose(\n","                Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n","                CenterCrop(size=(224, 224))\n","                ToTensor()\n","                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","            ), 'val': Dataset ImageFolder\n","     Number of datapoints: 24\n","     Root location: /content/drive/MyDrive/Deep Learning training data/val\n","     StandardTransform\n"," Transform: Compose(\n","                Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n","                CenterCrop(size=(224, 224))\n","                ToTensor()\n","                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","            )}"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"iYSEZtTDJUYV"},"source":["# Loading a pre-trained ResNet model\n","\n","Let's load a ResNet34 model from `torchvision` and examine it."]},{"cell_type":"code","metadata":{"id":"tOYRfm1pBcoM","colab":{"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["5deff87f65ce4fd28533c66e84702548","fea8a65789de444594527251bed8d116","ae4252c072a84be39fbf5e30402aacd5","df25980605cd41d5aea3ddcfe8e72b07","1a1e6942bce84b68b7d73e3258503b11","7010af1383484ac8897357d6209e4a18","5f36225fbc4343dc88da438a656eb3b3","89ee8c30bab34dcca776d4529cfa0c7d","e95b6d411c3a47ea8c0ccad2699330a2","cda7d8c7d6dd42d2a451b224d9a43976","5444e6cf6e5746a0b764a0826b8a1521"]},"outputId":"8031ed7a-ed5c-4e61-c603-75c46cd4b4b9","executionInfo":{"status":"ok","timestamp":1673550945289,"user_tz":300,"elapsed":1471,"user":{"displayName":"Sujay Banerjee","userId":"05482044087577137222"}}},"source":["# intialize model\n","model = models.resnet34(pretrained=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/83.3M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5deff87f65ce4fd28533c66e84702548"}},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mVgCL8ZTtvTt","outputId":"d79eadc5-66e2-4689-9e27-1c99f839a465","executionInfo":{"status":"ok","timestamp":1673550949787,"user_tz":300,"elapsed":276,"user":{"displayName":"Sujay Banerjee","userId":"05482044087577137222"}}},"source":["print(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (4): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (5): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"EfWGLTlJLxzI"},"source":["Hmm, lots of layers. Each `BasicBlock` is two or three conv layers with a skipped connection, with batch-norm layers thrown in for good measure. Several such residual blocks are pieced together, and in the end there is a dense layer with 1000 output neurons. This model has been trained on the well-known *ImageNet* dataset (which has over a million images with 1000 classes). As an aside, let's examine the number of trainable parameters in the model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jl9m-Z4EDFFW","outputId":"543aea52-0fe7-4b10-e3e0-2839e3db4adf","executionInfo":{"status":"ok","timestamp":1673550956435,"user_tz":300,"elapsed":211,"user":{"displayName":"Sujay Banerjee","userId":"05482044087577137222"}}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(count_parameters(model))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21797672\n"]}]},{"cell_type":"markdown","metadata":{"id":"FdzbKTClM_Qv"},"source":["Let us finetune this model for our cat-vs-dog classification problem. Since this is a binary classifier we will redefine the output (linear) layer to have 2 outputs."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qkjny7j5BDRS","outputId":"0f60c109-190d-4420-a6dc-67f0b72f38c4","executionInfo":{"status":"ok","timestamp":1673550958912,"user_tz":300,"elapsed":121,"user":{"displayName":"Sujay Banerjee","userId":"05482044087577137222"}}},"source":["num_ftrs = model.fc.in_features\n","print(num_ftrs)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["512\n"]}]},{"cell_type":"code","metadata":{"id":"nhnOEfZoDQS6"},"source":["model.fc = nn.Linear(num_ftrs, 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jv86CyS3DVe8","outputId":"be149191-e8fa-44d1-a6a8-92c3a94f0628","executionInfo":{"status":"ok","timestamp":1673550965601,"user_tz":300,"elapsed":124,"user":{"displayName":"Sujay Banerjee","userId":"05482044087577137222"}}},"source":["print(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (4): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (5): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=2, bias=True)\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"TIDpqRLJOMaJ"},"source":["Observe now that the basic ResNet34 backbone remains the same; only the output layer has changed. In fact, all of the weights (except the output layer) also have been retained.  \n","\n","Let's do a quick model evaluation to check if there are any errors thrown during prediction."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bq72K2tuDn3h","outputId":"cf257731-00de-4128-b98f-a86fc08b0199","executionInfo":{"status":"ok","timestamp":1673551830165,"user_tz":300,"elapsed":3671,"user":{"displayName":"Sujay Banerjee","userId":"05482044087577137222"}}},"source":["model.eval()\n","corrects = 0\n","for batch_idx, (inputs,labels) in enumerate(dataloaders['val'], 1):\n","  with torch.set_grad_enabled(False):\n","    outputs = model(inputs)\n","    _, preds = torch.max(outputs,1)\n","    \n","  corrects += torch.sum(preds == labels.data)\n","\n","print(corrects.float() / len(dataloaders['val'].dataset))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.)\n"]}]},{"cell_type":"markdown","metadata":{"id":"LI3eN9vcPEHb"},"source":["As we can see, we get an accuracy of 50\\% on the test set -- which is exactly what we would expect since the weights of the output layer are random. \n","\n","We are now ready to start fine-tuning! The rest of the code below is boilerplate training; we see below that only a few epochs are enough to tune the weights to our training dataset.  "]},{"cell_type":"code","metadata":{"id":"UZ8n_358A6aY"},"source":["# define loss function, optimizer\n","criterion = nn.CrossEntropyLoss() #because it can be 0 or 1 which is predicting categories\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","save_loss = {'train':[], 'val':[]}\n","save_acc = {'train':[], 'val':[]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJHYjaYtrNHm","outputId":"ef69dd9a-caa8-4c3e-f1cd-82b57f6106d8","executionInfo":{"status":"ok","timestamp":1673552484936,"user_tz":300,"elapsed":164740,"user":{"displayName":"Sujay Banerjee","userId":"05482044087577137222"}}},"source":["for epoch in range(5):\n","\n","    # Each epoch has a training and validation phase\n","    for phase in ['train', 'val']:\n","        if phase == 'train':\n","            model.train()  # Set model to training mode\n","        else:\n","            model.eval()   # Set model to evaluate mode\n","\n","        current_loss = 0.0\n","        current_corrects = 0\n","\n","        for batch_idx, (inputs, labels) in enumerate(dataloaders[phase], 1):\n","            optimizer.zero_grad()\n","\n","            # Time to carry out the forward training poss\n","            with torch.set_grad_enabled(phase == 'train'):\n","                outputs = model(inputs) #forward pass\n","                _, preds = torch.max(outputs, 1)\n","                loss = criterion(outputs, labels) # forward pass\n","\n","                # backward + optimize only if in training phase\n","                if phase == 'train':\n","                    loss.backward() #backward pass\n","                    optimizer.step() # weight updates\n","\n","            # We want variables to hold the loss/acc statistics\n","            current_loss += loss.item() * inputs.size(0)\n","            current_corrects += torch.sum(preds == labels.data)\n","        # saving variable for plottin\n","        save_loss[phase] += [current_loss / len(dataloaders[phase].dataset)]\n","        save_acc[phase] += [current_corrects.float() / len(dataloaders[phase].dataset)]\n","\n","        # pretty print\n","        print(f\"Epoch:{epoch} -- Phase:{phase} -- Loss:{save_loss[phase][-1]:.2f} -- Acc:{save_acc[phase][-1]*100:.2f}\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:0 -- Phase:train -- Loss:0.01 -- Acc:100.00\n","Epoch:0 -- Phase:val -- Loss:0.01 -- Acc:100.00\n","Epoch:1 -- Phase:train -- Loss:0.03 -- Acc:100.00\n","Epoch:1 -- Phase:val -- Loss:0.01 -- Acc:100.00\n","Epoch:2 -- Phase:train -- Loss:0.01 -- Acc:100.00\n","Epoch:2 -- Phase:val -- Loss:0.01 -- Acc:100.00\n","Epoch:3 -- Phase:train -- Loss:0.01 -- Acc:100.00\n","Epoch:3 -- Phase:val -- Loss:0.01 -- Acc:100.00\n","Epoch:4 -- Phase:train -- Loss:0.01 -- Acc:100.00\n","Epoch:4 -- Phase:val -- Loss:0.01 -- Acc:100.00\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"JGgbCwdTuy1h","outputId":"da4513a7-eeb0-4d32-8bf2-0e8fc0523f22","executionInfo":{"status":"ok","timestamp":1673552502991,"user_tz":300,"elapsed":523,"user":{"displayName":"Sujay Banerjee","userId":"05482044087577137222"}}},"source":["plt.plot(save_acc['train'])\n","plt.plot(save_acc['val'])\n","plt.legend([\"train\", \"test\"])\n","plt.title(\"Accuracy\")\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Accuracy')"]},"metadata":{},"execution_count":17},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"h1__s46hPeUg"},"source":["There we go! That took only 4 epochs of finetuning.\n","\n","This was a very small dataset (of only 60 training images) so it is not that surprising that we were able to fit the data so easily. Try training your own classifier with a slightly larger set of data points, and see if you can get similar results."]}]}