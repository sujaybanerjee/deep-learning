{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HZv7nTxXLZPe"},"outputs":[],"source":["#Import statements\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import functools\n","\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","from torch.nn.modules.activation import ReLU\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["batch_size = 12\n","IMG_SIZE = 572\n","\n","#A transform to resize, randomly flip, and scale images and convert them to tensors\n","transform = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","                                #transforms.RandomHorizontalFlip(), #Maybe don't do this?\n","                                transforms.ToTensor(), #Convert to tensor\n","                                transforms.Lambda(lambda t: (t * 2) - 1) #Scale between [-1, 1]]\n","])\n","\n","#Load data\n","dataset = MNIST('.', train=True, transform=transform, download=True)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)"],"metadata":{"id":"Zl6HiP9-Cacg","executionInfo":{"status":"ok","timestamp":1675095735411,"user_tz":300,"elapsed":552,"user":{"displayName":"Hayden Hunskor","userId":"13568985249816097194"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"76723939-243f-4811-bdc6-c0950a1ee621"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":["# #device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# # Simulate forward diffusion\n","# image, _ = next(iter(dataloader))\n","\n","# plt.figure(figsize=(15,15))\n","# plt.axis('off')\n","# num_images = 10\n","# stepsize = int(T/num_images)\n","\n","# for idx in range(0, T, stepsize):\n","#     t = torch.Tensor([idx]).type(torch.int64)\n","#     plt.subplot(1, num_images+1, (idx/stepsize) + 1)\n","#     image, noise = forward_diffusion_sample(image, t)\n","#     show_tensor_image(image)\n"],"metadata":{"id":"pE7kviROhTA5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#CODE TAKEN FROM https://colab.research.google.com/drive/1sjy9odlSSy0RBVgMTgP7s99NXsqglsUL?usp=sharing#scrollTo=Rj17psVw7Shg\n","class Diffusion:\n","    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256, device=\"cuda\"):\n","        self.noise_steps = noise_steps\n","        self.beta_start = beta_start\n","        self.beta_end = beta_end\n","        self.img_size = img_size\n","        self.device = device\n","\n","        self.beta = self.prepare_noise_schedule().to(device)\n","        self.alpha = 1. - self.beta\n","        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n","\n","    def prepare_noise_schedule(self):\n","        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n","\n","    def noise_images(self, x, t):\n","        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n","        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n","        Ɛ = torch.randn_like(x)\n","        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n","\n","    def sample_timesteps(self, n):\n","        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n","\n","    def sample(self, model, n):\n","        #logging.info(f\"Sampling {n} new images....\")\n","        model.eval()\n","        with torch.no_grad():\n","            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n","            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n","                t = (torch.ones(n) * i).long().to(self.device)\n","                predicted_noise = model(x, t)\n","                alpha = self.alpha[t][:, None, None, None]\n","                alpha_hat = self.alpha_hat[t][:, None, None, None]\n","                beta = self.beta[t][:, None, None, None]\n","                if i > 1:\n","                    noise = torch.randn_like(x)\n","                else:\n","                    noise = torch.zeros_like(x)\n","                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n","        model.train()\n","        x = (x.clamp(-1, 1) + 1) / 2\n","        x = (x * 255).type(torch.uint8)\n","        return x"],"metadata":{"id":"9OS3CY81gqz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Define UNet architecture\n","\n","#Double convolutional layer\n","#def double_conv(in_channels, out_channels, residual=False):\n","\n","# class double_conv(nn.Module):\n","#   def __init__(self, in_channels, out_channels, residual=False):\n","#         super().__init__()\n","#         self.residual = residual\n","#         self.conv_layer = nn.Sequential(\n","#             nn.Conv2d(in_channels, out_channels, kernel_size=3),\n","#             nn.ReLU(),\n","#             nn.Conv2d(out_channels, out_channels, kernel_size=3),\n","#             nn.ReLU())\n","  \n","#   def forward(self, x):\n","#         if self.residual:\n","#             return F.gelu(x + self.conv_layer(x))\n","#         else:\n","#             return self.conv_layer(x)\n","\n","def double_conv(in_channels, out_channels):\n","  return nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3),\n","            nn.ReLU(),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n","            nn.ReLU()\n","          )\n","\n","#Crop function \n","def img_crop(tensor, target_tensor):\n","  tensor_size = tensor.size()[2]\n","  target_size = target_tensor.size()[2]\n","  delta = tensor_size - target_size\n","  delta = delta // 2\n","\n","  return tensor[:,:, delta:tensor_size-delta,delta:tensor_size-delta]\n","\n","#Class for an up-sampling block that handles skip connections\n","class Up(nn.Module):\n","    def __init__(self, in_channels, out_channels, emb_dim=256):\n","        super().__init__()\n","\n","        #self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","\n","        self.up_trans = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n","        self.up_conv = double_conv(in_channels, out_channels)\n","\n","        self.emb_layer = nn.Sequential(\n","            #nn.SiLU(),\n","            nn.Linear(\n","                emb_dim,\n","                out_channels\n","            ),\n","        )\n","\n","    def forward(self, x, skip_x, t):\n","        x = self.up_trans(x)\n","        skip_x = img_crop(skip_x, x)\n","        #print(x.shape)\n","        #print(skip_x.shape)\n","        x = torch.cat([skip_x, x], dim=1)\n","        x = self.up_conv(x)\n","        print(x.shape)\n","        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n","        print(emb.shape)\n","        return x + emb\n","\n","#Class for a down-sampling block\n","class Down(nn.Module):\n","    def __init__(self, in_channels, out_channels, emb_dim=256):\n","        super().__init__()\n","        self.img_layer = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            double_conv(in_channels, out_channels)\n","        )\n","\n","        self.emb_layer = nn.Sequential(\n","            #nn.SiLU(),\n","            nn.Linear(\n","                emb_dim,\n","                out_channels\n","            ),\n","        )\n","\n","    def forward(self, x, t):\n","        x = self.img_layer(x)\n","        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1]) #don't understand this\n","        return x + emb\n","\n","#Class for the initial convolutional layer for the image\n","class Init_Conv(nn.Module):\n","  def __init__(self, in_channels, out_channels, emb_dim=256):\n","        super().__init__()\n","        self.conv = double_conv(in_channels, out_channels)\n","\n","        self.emb_layer = nn.Sequential(\n","            #nn.SiLU(),\n","            nn.Linear(\n","                emb_dim,\n","                out_channels\n","            ),\n","        )\n","\n","  def forward(self, x, t):\n","        x = self.conv(x)\n","        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1]) #don't understand this\n","        return x + emb\n","\n","#UNet model architecture\n","class UNet(nn.Module):\n","  def __init__(self, time_dim=256):\n","    super(UNet, self).__init__()\n","    self.time_dim = time_dim\n","\n","    #Initial convolutional layer\n","    #self.conv1 = double_conv(1, 64)\n","    self.conv1 = Init_Conv(1,64)\n","\n","    #4 down-sampling blocks that handle img and timestep\n","    self.down1 = Down(64, 128)\n","    self.down2 = Down(128, 256)\n","    self.down3 = Down(256, 512)\n","    self.down4 = Down(512, 1024)\n","\n","    #4 up-sampling blocks that handle img and timestep\n","    self.up1 = Up(1024, 512)\n","    self.up2 = Up(512, 256)\n","    self.up3 = Up(256, 128)\n","    self.up4 = Up(128, 64)\n","\n","    #Final convolutional layer\n","    self.last_conv = nn.Conv2d(64, 1, kernel_size=1, stride=1)\n","\n","  def pos_encoding(self, t, channels):\n","    inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2)))#, device=one_param(self).device).float() / channels))\n","    pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n","    pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n","    pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n","    \n","    return pos_enc\n","\n","  def forward(self, img, t):\n","\n","    t = t.unsqueeze(-1)\n","    t = self.pos_encoding(t, self.time_dim)\n","\n","    #Encoder\n","    #x1 = self.conv1(img)\n","    x1 = self.conv1(img, t)\n","    x2 = self.down1(x1, t)\n","    x3 = self.down2(x2, t)\n","    x4 = self.down3(x3, t)\n","    x5 = self.down4(x4, t)\n","\n","    ## COULD ADD A BOTTLENECK WITH EXTRA CONV LAYERS ##\n","\n","    #Decoder\n","    x = self.up1(x5, x4, t)\n","    x = self.up2(x, x3, t)\n","    x = self.up3(x, x2, t)\n","    #print(x.shape)\n","    #print(x1.shape)\n","    #x1 = torch.cat((t, x1), dim=0)\n","    #print(x1.shape)\n","    #print(x1.unsqueeze(0).shape)\n","    x = self.up4(x, x1, t)\n","    out = self.last_conv(x)\n","\n","    return(out)\n","    \n"],"metadata":{"id":"qi2VGkbW9E8R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images, labels = next(iter(dataloader))\n","model = UNet()\n","\n","#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","def sample_timesteps(n):\n","    return torch.randint(low=1, high=50, size=(n,))\n","\n","t = sample_timesteps(images.shape[0])#.to(device)\n","#print(t.shape)\n","\n","print(model(images[0], t))\n","\n","\n","#plt.imshow(model(images[0]).squeeze(), cmap='gray')\n","\n","\n","#print(model(images[0]).shape)"],"metadata":{"id":"YuYjpAebHSP8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(images[0].shape.length)"],"metadata":{"id":"Di_uqWo3WYJe"},"execution_count":null,"outputs":[]}]}